import os
import shutil
from pathlib import Path
import pandas as pd
from PIL import Image
import imagehash
from tqdm import tqdm
from datasets import load_dataset
import gdown
import subprocess
import requests


RAW_DIR = Path("./datasets/raw")
DATASET_LINKS = [r"/mnt/d/picture",
                "https://huggingface.co/datasets/PedroSampaio/fruits-360",
                "https://www.kaggle.com/datasets/lakshaytyagi01/fruit-detection"]

def download_http(url, save_dir):
    filename = os.path.basename(url)
    r = requests.get(url, stream=True)
    if r.status_code == 200:
        with open(save_dir / filename, 'wb') as f:
            shutil.copyfileobj(r.raw, f)
        return True
    return False

from datasets import load_dataset, concatenate_datasets
from PIL import Image

def download_huggingface(url, save_dir):
    """
    T·∫£i dataset t·ª´ Hugging Face b·∫±ng datasets.load_dataset.
    - url c√≥ th·ªÉ l√† "https://huggingface.co/datasets/owner/repo" ho·∫∑c "owner/repo".
    - L∆∞u t·∫•t c·∫£ ·∫£nh v√†o save_dir/<label>/...
    - Tr·∫£ v·ªÅ True/False.
    """
    try:
        # 1) Chu·∫©n h√≥a dataset_id t·ª´ URL ho·∫∑c tr·∫£ nguy√™n n·∫øu ƒë√£ l√† "owner/repo"
        if "huggingface.co" in url:
            if "/datasets/" in url:
                dataset_id = url.split("/datasets/")[-1].strip("/")
            else:
                # d·ª± ph√≤ng: l·∫•y ph·∫ßn cu·ªëi l√†m id
                dataset_id = url.rstrip("/").split("/")[-1]
        else:
            dataset_id = url

        print(f"üîΩ ƒêang t·∫£i dataset Hugging Face: {dataset_id}")

        # 2) Load dataset (t·∫£i t·∫•t c·∫£ splits)
        dataset = load_dataset(dataset_id)
        # N·∫øu tr·∫£ v·ªÅ DatasetDict (multiple splits), g·ªôp l·∫°i
        if isinstance(dataset, dict) or hasattr(dataset, "keys"):
            # dataset c√≥ th·ªÉ l√† DatasetDict ‚Äî g·ªôp t·∫•t c·∫£ split th√†nh 1 Dataset
            try:
                dataset = concatenate_datasets([dataset[s] for s in dataset.keys()])
            except Exception as e:
                print("‚ö†Ô∏è Kh√¥ng th·ªÉ concatenate splits tr·ª±c ti·∫øp:", e)
                # fallback: n·∫øu dataset l√† DatasetDict, l·∫∑p t·ª´ng split l∆∞u ·∫£nh t·ª´ng split
                # (nh∆∞ng ·ªü ƒë√¢y ta c·ªë g·∫Øng g·ªôp; ch·ªâ proceed n·∫øu g·ªôp ƒë∆∞·ª£c)
                raise

        # 3) ƒë·∫£m b·∫£o th∆∞ m·ª•c ƒë√≠ch t·ªìn t·∫°i
        save_dir.mkdir(parents=True, exist_ok=True)

        # 4) l∆∞u ·∫£nh: x·ª≠ l√Ω nhi·ªÅu d·∫°ng field 'image'
        saved = 0
        for i, sample in enumerate(dataset):
            try:
                # C√≥ th·ªÉ image c√≥ ·ªü key 'image' ho·∫∑c 'img' ‚Äî th·ª≠ nhi·ªÅu kh·∫£ nƒÉng
                img_field = None
                for key in ("image", "img", "image_path", "file"):
                    if key in sample:
                        img_field = sample[key]
                        break

                if img_field is None:
                    # N·∫øu kh√¥ng c√≥ key image, b·ªè qua
                    print(f"‚ö†Ô∏è M·∫´u {i} kh√¥ng c√≥ tr∆∞·ªùng ·∫£nh, b·ªè qua.")
                    continue

                # N·∫øu l√† PIL Image
                if isinstance(img_field, Image.Image):
                    img = img_field
                else:
                    # C√≥ th·ªÉ l√† datasets Image (PILWrapper) -> t·∫£i b·∫±ng .to_pil()
                    try:
                        img = img_field.to_pil()  # m·ªôt s·ªë version h·ªó tr·ª£
                    except Exception:
                        # C√≥ th·ªÉ l√† dict ch·ª©a 'path' ho·∫∑c bytes
                        if isinstance(img_field, dict) and "path" in img_field:
                            img = Image.open(img_field["path"])
                        elif isinstance(img_field, str) and os.path.exists(img_field):
                            img = Image.open(img_field)
                        else:
                            # cu·ªëi c√πng, th·ª≠ convert tr·ª±c ti·∫øp (n·∫øu l√† numpy array)
                            import numpy as np
                            if isinstance(img_field, np.ndarray):
                                img = Image.fromarray(img_field)
                            else:
                                print(f"‚ö†Ô∏è Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c d·∫°ng ·∫£nh cho m·∫´u {i}, b·ªè qua.")
                                continue

                # ƒë·∫£m b·∫£o RGB
                if img.mode != "RGB":
                    img = img.convert("RGB")

                # label (n·∫øu c√≥)
                label = str(sample.get("label", "unknown"))
                label_dir = save_dir / label
                label_dir.mkdir(parents=True, exist_ok=True)

                # t√™n file: dataset_id an to√†n (thay / th√†nh _)
                safe_dataset_id = dataset_id.replace("/", "_").replace(" ", "_")
                file_name = f"{safe_dataset_id}_{i:06d}.jpg"
                out_path = label_dir / file_name

                # l∆∞u ·∫£nh (ƒë·∫£m b·∫£o th∆∞ m·ª•c cha ƒë√£ t·ªìn t·∫°i)
                out_path.parent.mkdir(parents=True, exist_ok=True)
                img.save(out_path)
                saved += 1

            except Exception as e:
                print(f"‚ùå L·ªói l∆∞u ·∫£nh m·∫´u {i}: {e}")
                continue

        print(f"‚úÖ Ho√†n t·∫•t: ƒë√£ l∆∞u {saved} ·∫£nh t·ª´ {dataset_id} v√†o {save_dir}")
        return True

    except Exception as e:
        print("‚ùå HF error:", e)
        return False

def download_kaggle(url, save_dir):
    try:
        parts = url.split("/datasets/")[-1]
        subprocess.run(["kaggle", "datasets", "download", "-d", parts, "-p", str(save_dir)], check=True)
        return True
    except Exception as e:
        print("‚ùå Kaggle error:", e)
        return False

def download_gdrive(url, save_dir):
    try:
        gdown.download(url, output=str(save_dir), quiet=False, fuzzy=True)
        return True
    except Exception as e:
        print("‚ùå Drive error:", e)
        return False

def copy_local(path, save_dir):
    p = Path(path)
    if p.is_dir():
        for f in p.rglob("*.*"):
            shutil.copy(f, save_dir / f.name)
    elif p.is_file():
        shutil.copy(p, save_dir / p.name)

def detect_source(link):
    if "huggingface.co" in link:
        return "huggingface"
    elif "kaggle.com" in link:
        return "kaggle"
    elif "drive.google.com" in link:
        return "gdrive"
    elif link.startswith("http"):
        return "http"
    else:
        return "local"

def remove_duplicates(folder):
    print("üßπ ƒêang ki·ªÉm tra ·∫£nh tr√πng l·∫∑p...")
    hashes = {}
    removed = 0

    for img_path in tqdm(list(Path(folder).rglob("*.jpg")), desc="ƒêang qu√©t ·∫£nh"):
        try:
            img = Image.open(img_path)
            img_hash = imagehash.phash(img)

            if img_hash in hashes:
                # N·∫øu hash ƒë√£ t·ªìn t·∫°i ‚Üí ·∫£nh tr√πng ‚Üí xo√° ƒëi
                os.remove(img_path)
                removed += 1
            else:
                hashes[img_hash] = img_path
        except Exception as e:
            print(f"L·ªói khi x·ª≠ l√Ω {img_path}: {e}")

    print(f"‚úÖ ƒê√£ xo√° {removed} ·∫£nh tr√πng l·∫∑p")

def main():
    for link in tqdm(DATASET_LINKS, desc="ƒêang x·ª≠ l√Ω dataset"):
        source = detect_source(link)
        print(f"üì¶ Ngu·ªìn: {source} ‚Üí {link}")

        if source == "huggingface":
            download_huggingface(link, RAW_DIR)
        elif source == "kaggle":
            download_kaggle(link, RAW_DIR)
        elif source == "gdrive":
            download_gdrive(link, RAW_DIR)
        elif source == "http":
            download_http(link, RAW_DIR)
        elif source == "local":
            copy_local(link, RAW_DIR)
    remove_duplicates(RAW_DIR)

if __name__ == "__main__":
        main()